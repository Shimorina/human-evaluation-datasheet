@inproceedings{banarescu-etal-2013-abstract,
    title = "{A}bstract {M}eaning {R}epresentation for Sembanking",
    author = "Banarescu, Laura  and
      Bonial, Claire  and
      Cai, Shu  and
      Georgescu, Madalina  and
      Griffitt, Kira  and
      Hermjakob, Ulf  and
      Knight, Kevin  and
      Koehn, Philipp  and
      Palmer, Martha  and
      Schneider, Nathan",
    booktitle = "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W13-2322",
    pages = "178--186",
}

@article{gebru2018datasheets,
      title={Datasheets for Datasets}, 
      author={Timnit Gebru and Jamie Morgenstern and Briana Vecchione and Jennifer Wortman Vaughan and Hanna Wallach and Hal Daumé III and Kate Crawford},
      year={2020},
      eprint={1803.09010},
      archivePrefix={arXiv},
      primaryClass={cs.DB}
}

@article{bender-friedman-2018-data,
    title = "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science",
    author = "Bender, Emily M.  and
      Friedman, Batya",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    url = "https://www.aclweb.org/anthology/Q18-1041",
    doi = "10.1162/tacl_a_00041",
    pages = "587--604",
    abstract = "In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",
}

@book{kamp-reyle2013discourse,
  title={From discourse to logic: Introduction to modeltheoretic semantics of natural language, formal logic and discourse representation theory},
  author={Kamp, Hans and Reyle, Uwe},
  volume={42},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@inproceedings{mitchell2019modelcards,
author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
title = {Model Cards for Model Reporting},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287596},
doi = {10.1145/3287560.3287596},
abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {220–229},
numpages = {10},
keywords = {ethical considerations, documentation, datasheets, model cards, fairness evaluation, disaggregated evaluation, ML model evaluation},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{card-etal-2020-little,
    title = "With Little Power Comes Great Responsibility",
    author = "Card, Dallas  and
      Henderson, Peter  and
      Khandelwal, Urvashi  and
      Jia, Robin  and
      Mahowald, Kyle  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.745",
    doi = "10.18653/v1/2020.emnlp-main.745",
    pages = "9263--9274",
    abstract = "Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings. By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75{\%} power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses.",
}

@inproceedings{van2019best,
  title={Best practices for the human evaluation of automatically generated text},
  author={{van der Lee}, Chris and Gatt, Albert and van Miltenburg, Emiel and Wubben, Sander and Krahmer, Emiel},
  booktitle={Proceedings of the 12th International Conference on Natural Language Generation},
  pages={355--368},
  url={https://www.aclweb.org/anthology/W19-8643.pdf},
  year={2019}
}

@article{vanderlee2021,
title = {Human evaluation of automatically generated text: Current trends and best practice guidelines},
journal = {Computer Speech \& Language},
volume = {67},
pages = {101151},
year = {2021},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2020.101151},
url = {https://www.sciencedirect.com/science/article/pii/S088523082030084X},
author = {Chris {van der Lee} and Albert Gatt and Emiel {van Miltenburg} and Emiel Krahmer},
keywords = {Natural Language Generation, Human evaluation, Recommendations, Literature review, Open science, Ethics},
abstract = {Currently, there is little agreement as to how Natural Language Generation (NLG) systems should be evaluated, with a particularly high degree of variation in the way that human evaluation is carried out. This paper provides an overview of how (mostly intrinsic) human evaluation is currently conducted and presents a set of best practices, grounded in the literature. These best practices are also linked to the stages that researchers go through when conducting an evaluation research (planning stage; execution and release stage), and the specific steps in these stages. With this paper, we hope to contribute to the quality and consistency of human evaluations in NLG.}
}

@article{gehrmann2021gem,
      title={The {GEM} Benchmark: Natural Language Generation, its Evaluation and Metrics}, 
      author={Sebastian Gehrmann and Tosin Adewumi and Karmanya Aggarwal and Pawan Sasanka Ammanamanchi and Aremu Anuoluwapo and Antoine Bosselut and Khyathi Raghavi Chandu and Miruna Clinciu and Dipanjan Das and Kaustubh D. Dhole and Wanyu Du and Esin Durmus and Ondřej Dušek and Chris Emezue and Varun Gangal and Cristina Garbacea and Tatsunori Hashimoto and Yufang Hou and Yacine Jernite and Harsh Jhamtani and Yangfeng Ji and Shailza Jolly and Dhruv Kumar and Faisal Ladhak and Aman Madaan and Mounica Maddela and Khyati Mahajan and Saad Mahamood and Bodhisattwa Prasad Majumder and Pedro Henrique Martins and Angelina McMillan-Major and Simon Mille and Emiel van Miltenburg and Moin Nadeem and Shashi Narayan and Vitaly Nikolaev and Rubungo Andre Niyongabo and Salomey Osei and Ankur Parikh and Laura Perez-Beltrachini and Niranjan Ramesh Rao and Vikas Raunak and Juan Diego Rodriguez and Sashank Santhanam and João Sedoc and Thibault Sellam and Samira Shaikh and Anastasia Shimorina and Marco Antonio Sobrevilla Cabezudo and Hendrik Strobelt and Nishant Subramani and Wei Xu and Diyi Yang and Akhila Yerukola and Jiawei Zhou},
      year={2021},
      eprint={2102.01672},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{howcroft-etal-2020-twenty,
    title = "Twenty Years of Confusion in Human Evaluation: {NLG} Needs Evaluation Sheets and Standardised Definitions",
    author = "Howcroft, David M.  and
      Belz, Anya  and
      Clinciu, Miruna-Adriana  and
      Gkatzia, Dimitra  and
      Hasan, Sadid A.  and
      Mahamood, Saad  and
      Mille, Simon  and
      van Miltenburg, Emiel  and
      Santhanam, Sashank  and
      Rieser, Verena",
    booktitle = "Proceedings of the 13th International Conference on Natural Language Generation",
    month = dec,
    year = "2020",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.inlg-1.23",
    pages = "169--182"
}

@inproceedings{belz-etal-2020-disentangling,
    title = "Disentangling the Properties of Human Evaluation Methods: A Classification System to Support Comparability, Meta-Evaluation and Reproducibility Testing",
    author = "Belz, Anya  and
      Mille, Simon  and
      Howcroft, David M.",
    booktitle = "Proceedings of the 13th International Conference on Natural Language Generation",
    month = dec,
    year = "2020",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.inlg-1.24",
    pages = "183--194"
}

@misc{shimorina-belz-2021-heds,
      title={The Human Evaluation Datasheet 1.0: A Template for Recording Details of Human Evaluation Experiments in NLP}, 
      author={Anastasia Shimorina and Anya Belz},
      year={2021},
      eprint={2103.09710},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
